{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "08ac061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "import torch.utils.data.distributed\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fc996b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoader(object):\n",
    "    def __init__(self, fldr_path):\n",
    "        self.testing_samples = CustomLoadPreprocess(fldr_path)\n",
    "        self.data = DataLoader(self.testing_samples, 1,\n",
    "                               shuffle=False,\n",
    "                               pin_memory=False)\n",
    "        print(\"CustomLoader Ctor: \")\n",
    "\n",
    "\n",
    "class CustomLoadPreprocess(Dataset):\n",
    "    def __init__(self, fldr_path):\n",
    "        print(\"CustomLoadPreprocess Ctor: \")\n",
    "\n",
    "        self.fldr_path = fldr_path\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.filenames = glob.glob(self.fldr_path + '/*.png') + glob.glob(self.fldr_path + '/*.jpg')\n",
    "        self.input_height = 480#160\n",
    "        self.input_width = 640#371\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.filenames[idx]\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "        img = img.convert(\"RGB\").resize(size=(self.input_width, self.input_height), resample=Image.BILINEAR)\n",
    "        img = np.array(img).astype(np.float32) / 255.0\n",
    "\n",
    "        print(\"Shape:\", img.shape)\n",
    "        img = torch.from_numpy(img).permute(2, 0, 1)\n",
    "        print(\"Shape3:\", img.shape)\n",
    "\n",
    "        img = self.normalize(img)\n",
    "        print(\"Shape4:\", img.shape)\n",
    "\n",
    "        img_name = img_path.split('/')[-1]\n",
    "        img_name = img_name.split('.png')[0] if '.png' in img_name else img_name.split('.jpg')[0]\n",
    "\n",
    "        sample = {'img': img,\n",
    "                  'img_name': img_name}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c1f9acaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Upsample + BatchNorm\n",
    "class UpSampleBN(nn.Module):\n",
    "    def __init__(self, skip_input, output_features):\n",
    "        super(UpSampleBN, self).__init__()\n",
    "\n",
    "        self._net = nn.Sequential(nn.Conv2d(skip_input, output_features, kernel_size=3, stride=1, padding=1),\n",
    "                                  nn.BatchNorm2d(output_features),\n",
    "                                  nn.LeakyReLU(),\n",
    "                                  nn.Conv2d(output_features, output_features, kernel_size=3, stride=1, padding=1),\n",
    "                                  nn.BatchNorm2d(output_features),\n",
    "                                  nn.LeakyReLU())\n",
    "\n",
    "    def forward(self, x, concat_with):\n",
    "        up_x = F.interpolate(x, size=[concat_with.size(2), concat_with.size(3)], mode='bilinear', align_corners=True)\n",
    "        f = torch.cat([up_x, concat_with], dim=1)\n",
    "        return self._net(f)\n",
    "\n",
    "\n",
    "# Upsample + GroupNorm + Weight Standardization\n",
    "class UpSampleGN(nn.Module):\n",
    "    def __init__(self, skip_input, output_features):\n",
    "        super(UpSampleGN, self).__init__()\n",
    "\n",
    "        self._net = nn.Sequential(Conv2d(skip_input, output_features, kernel_size=3, stride=1, padding=1),\n",
    "                                  nn.GroupNorm(8, output_features),\n",
    "                                  nn.LeakyReLU(),\n",
    "                                  Conv2d(output_features, output_features, kernel_size=3, stride=1, padding=1),\n",
    "                                  nn.GroupNorm(8, output_features),\n",
    "                                  nn.LeakyReLU())\n",
    "\n",
    "    def forward(self, x, concat_with):\n",
    "        up_x = F.interpolate(x, size=[concat_with.size(2), concat_with.size(3)], mode='bilinear', align_corners=True)\n",
    "        f = torch.cat([up_x, concat_with], dim=1)\n",
    "        return self._net(f)\n",
    "\n",
    "\n",
    "# Conv2d with weight standardization\n",
    "class Conv2d(nn.Conv2d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=True):\n",
    "        super(Conv2d, self).__init__(in_channels, out_channels, kernel_size, stride,\n",
    "                 padding, dilation, groups, bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        weight = self.weight\n",
    "        weight_mean = weight.mean(dim=1, keepdim=True).mean(dim=2,\n",
    "                                  keepdim=True).mean(dim=3, keepdim=True)\n",
    "        weight = weight - weight_mean\n",
    "        std = weight.view(weight.size(0), -1).std(dim=1).view(-1, 1, 1, 1) + 1e-5\n",
    "        weight = weight / std.expand_as(weight)\n",
    "        return F.conv2d(x, weight, self.bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "\n",
    "\n",
    "# normalize\n",
    "def norm_normalize(norm_out):\n",
    "    min_kappa = 0.01\n",
    "    norm_x, norm_y, norm_z, kappa = torch.split(norm_out, 1, dim=1)\n",
    "    norm = torch.sqrt(norm_x ** 2.0 + norm_y ** 2.0 + norm_z ** 2.0) + 1e-10\n",
    "    kappa = F.elu(kappa) + 1.0 + min_kappa\n",
    "    final_out = torch.cat([norm_x / norm, norm_y / norm, norm_z / norm, kappa], dim=1)\n",
    "    return final_out\n",
    "\n",
    "\n",
    "# uncertainty-guided sampling (only used during training)\n",
    "@torch.no_grad()\n",
    "def sample_points(init_normal, gt_norm_mask, sampling_ratio, beta):\n",
    "    device = init_normal.device\n",
    "    B, _, H, W = init_normal.shape\n",
    "    N = int(sampling_ratio * H * W)\n",
    "    beta = beta\n",
    "\n",
    "    # uncertainty map\n",
    "    uncertainty_map = -1 * init_normal[:, 3, :, :]  # B, H, W\n",
    "\n",
    "    # gt_invalid_mask (B, H, W)\n",
    "    if gt_norm_mask is not None:\n",
    "        gt_invalid_mask = F.interpolate(gt_norm_mask.float(), size=[H, W], mode='nearest')\n",
    "        gt_invalid_mask = gt_invalid_mask[:, 0, :, :] < 0.5\n",
    "        uncertainty_map[gt_invalid_mask] = -1e4\n",
    "\n",
    "    # (B, H*W)\n",
    "    _, idx = uncertainty_map.view(B, -1).sort(1, descending=True)\n",
    "\n",
    "    # importance sampling\n",
    "    if int(beta * N) > 0:\n",
    "        importance = idx[:, :int(beta * N)]    # B, beta*N\n",
    "\n",
    "        # remaining\n",
    "        remaining = idx[:, int(beta * N):]     # B, H*W - beta*N\n",
    "\n",
    "        # coverage\n",
    "        num_coverage = N - int(beta * N)\n",
    "\n",
    "        if num_coverage <= 0:\n",
    "            samples = importance\n",
    "        else:\n",
    "            coverage_list = []\n",
    "            for i in range(B):\n",
    "                idx_c = torch.randperm(remaining.size()[1])    # shuffles \"H*W - beta*N\"\n",
    "                coverage_list.append(remaining[i, :][idx_c[:num_coverage]].view(1, -1))     # 1, N-beta*N\n",
    "            coverage = torch.cat(coverage_list, dim=0)                                      # B, N-beta*N\n",
    "            samples = torch.cat((importance, coverage), dim=1)                              # B, N\n",
    "\n",
    "    else:\n",
    "        # remaining\n",
    "        remaining = idx[:, :]  # B, H*W\n",
    "\n",
    "        # coverage\n",
    "        num_coverage = N\n",
    "\n",
    "        coverage_list = []\n",
    "        for i in range(B):\n",
    "            idx_c = torch.randperm(remaining.size()[1])  # shuffles \"H*W - beta*N\"\n",
    "            coverage_list.append(remaining[i, :][idx_c[:num_coverage]].view(1, -1))  # 1, N-beta*N\n",
    "        coverage = torch.cat(coverage_list, dim=0)  # B, N-beta*N\n",
    "        samples = coverage\n",
    "\n",
    "    # point coordinates\n",
    "    rows_int = samples // W         # 0 for first row, H-1 for last row\n",
    "    rows_float = rows_int / float(H-1)         # 0 to 1.0\n",
    "    rows_float = (rows_float * 2.0) - 1.0       # -1.0 to 1.0\n",
    "\n",
    "    cols_int = samples % W          # 0 for first column, W-1 for last column\n",
    "    cols_float = cols_int / float(W-1)         # 0 to 1.0\n",
    "    cols_float = (cols_float * 2.0) - 1.0       # -1.0 to 1.0\n",
    "\n",
    "    point_coords = torch.zeros(B, 1, N, 2)\n",
    "    point_coords[:, 0, :, 0] = cols_float             # x coord\n",
    "    point_coords[:, 0, :, 1] = rows_float             # y coord\n",
    "    point_coords = point_coords.to(device)\n",
    "    return point_coords, rows_int, cols_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5073c0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        basemodel_name = 'tf_efficientnet_b5_ap'\n",
    "        print('Loading base model ()...'.format(basemodel_name), end='')\n",
    "        basemodel = torch.hub.load('rwightman/gen-efficientnet-pytorch', basemodel_name, pretrained=True)\n",
    "        print('Done.')\n",
    "\n",
    "        # Remove last layer\n",
    "        print('Removing last two layers (global_pool & classifier).')\n",
    "        basemodel.global_pool = nn.Identity()\n",
    "        basemodel.classifier = nn.Identity()\n",
    "\n",
    "        self.original_model = basemodel\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = [x]\n",
    "        for k, v in self.original_model._modules.items():\n",
    "            if (k == 'blocks'):\n",
    "                for ki, vi in v._modules.items():\n",
    "                    features.append(vi(features[-1]))\n",
    "            else:\n",
    "                features.append(v(features[-1]))\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "701c8986",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # hyper-parameter for sampling\n",
    "        self.sampling_ratio = 0.4\n",
    "        self.importance_ratio = 0.7\n",
    "\n",
    "        # feature-map\n",
    "        self.conv2 = nn.Conv2d(2048, 2048, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "        self.up1 = UpSampleBN(skip_input=2048 + 176, output_features=1024)\n",
    "        self.up2 = UpSampleBN(skip_input=1024 + 64, output_features=512)\n",
    "        self.up3 = UpSampleBN(skip_input=512 + 40, output_features=256)\n",
    "        self.up4 = UpSampleBN(skip_input=256 + 24, output_features=128)\n",
    "\n",
    "        # produces 1/8 res output\n",
    "        self.out_conv_res8 = nn.Conv2d(512, 4, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # produces 1/4 res output\n",
    "        self.out_conv_res4 = nn.Sequential(\n",
    "            nn.Conv1d(512 + 4, 128, kernel_size=1), nn.ReLU(),\n",
    "            nn.Conv1d(128, 128, kernel_size=1), nn.ReLU(),\n",
    "            nn.Conv1d(128, 128, kernel_size=1), nn.ReLU(),\n",
    "            nn.Conv1d(128, 4, kernel_size=1),\n",
    "        )\n",
    "\n",
    "        # produces 1/2 res output\n",
    "        self.out_conv_res2 = nn.Sequential(\n",
    "            nn.Conv1d(256 + 4, 128, kernel_size=1), nn.ReLU(),\n",
    "            nn.Conv1d(128, 128, kernel_size=1), nn.ReLU(),\n",
    "            nn.Conv1d(128, 128, kernel_size=1), nn.ReLU(),\n",
    "            nn.Conv1d(128, 4, kernel_size=1),\n",
    "        )\n",
    "\n",
    "        # produces 1/1 res output\n",
    "        self.out_conv_res1 = nn.Sequential(\n",
    "            nn.Conv1d(128 + 4, 128, kernel_size=1), nn.ReLU(),\n",
    "            nn.Conv1d(128, 128, kernel_size=1), nn.ReLU(),\n",
    "            nn.Conv1d(128, 128, kernel_size=1), nn.ReLU(),\n",
    "            nn.Conv1d(128, 4, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, features, gt_norm_mask=None, mode='test'):\n",
    "        x_block0, x_block1, x_block2, x_block3, x_block4 = features[4], features[5], features[6], features[8], features[11]\n",
    "\n",
    "        # generate feature-map\n",
    "        x_d0 = self.conv2(x_block4)                     # x_d0 : [2, 2048, 15, 20]      1/32 res\n",
    "        x_d1 = self.up1(x_d0, x_block3)                 # x_d1 : [2, 1024, 30, 40]      1/16 res\n",
    "        x_d2 = self.up2(x_d1, x_block2)                 # x_d2 : [2, 512, 60, 80]       1/8 res\n",
    "        x_d3 = self.up3(x_d2, x_block1)                 # x_d3: [2, 256, 120, 160]      1/4 res\n",
    "        x_d4 = self.up4(x_d3, x_block0)                 # x_d4: [2, 128, 240, 320]      1/2 res\n",
    "\n",
    "        # 1/8 res output\n",
    "        out_res8 = self.out_conv_res8(x_d2)             # out_res8: [2, 4, 60, 80]      1/8 res output\n",
    "        out_res8 = norm_normalize(out_res8)             # out_res8: [2, 4, 60, 80]      1/8 res output\n",
    "\n",
    "        ################################################################################################################\n",
    "        # out_res4\n",
    "        ################################################################################################################\n",
    "\n",
    "        if mode == 'train':\n",
    "            # upsampling ... out_res8: [2, 4, 60, 80] -> out_res8_res4: [2, 4, 120, 160]\n",
    "            out_res8_res4 = F.interpolate(out_res8, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            B, _, H, W = out_res8_res4.shape\n",
    "\n",
    "            # samples: [B, 1, N, 2]\n",
    "            point_coords_res4, rows_int, cols_int = sample_points(out_res8_res4.detach(), gt_norm_mask,\n",
    "                                                                  sampling_ratio=self.sampling_ratio,\n",
    "                                                                  beta=self.importance_ratio)\n",
    "\n",
    "            # output (needed for evaluation / visualization)\n",
    "            out_res4 = out_res8_res4\n",
    "\n",
    "            # grid_sample feature-map\n",
    "            feat_res4 = F.grid_sample(x_d2, point_coords_res4, mode='bilinear', align_corners=True)  # (B, 512, 1, N)\n",
    "            init_pred = F.grid_sample(out_res8, point_coords_res4, mode='bilinear', align_corners=True)  # (B, 4, 1, N)\n",
    "            feat_res4 = torch.cat([feat_res4, init_pred], dim=1)  # (B, 512+4, 1, N)\n",
    "\n",
    "            # prediction (needed to compute loss)\n",
    "            samples_pred_res4 = self.out_conv_res4(feat_res4[:, :, 0, :])  # (B, 4, N)\n",
    "            samples_pred_res4 = norm_normalize(samples_pred_res4)  # (B, 4, N) - normalized\n",
    "\n",
    "            for i in range(B):\n",
    "                out_res4[i, :, rows_int[i, :], cols_int[i, :]] = samples_pred_res4[i, :, :]\n",
    "\n",
    "        else:\n",
    "            # grid_sample feature-map\n",
    "            feat_map = F.interpolate(x_d2, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            init_pred = F.interpolate(out_res8, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            feat_map = torch.cat([feat_map, init_pred], dim=1)  # (B, 512+4, H, W)\n",
    "            B, _, H, W = feat_map.shape\n",
    "\n",
    "            # try all pixels\n",
    "            out_res4 = self.out_conv_res4(feat_map.view(B, 512 + 4, -1))  # (B, 4, N)\n",
    "            out_res4 = norm_normalize(out_res4)  # (B, 4, N) - normalized\n",
    "            out_res4 = out_res4.view(B, 4, H, W)\n",
    "            samples_pred_res4 = point_coords_res4 = None\n",
    "\n",
    "        ################################################################################################################\n",
    "        # out_res2\n",
    "        ################################################################################################################\n",
    "\n",
    "        if mode == 'train':\n",
    "\n",
    "            # upsampling ... out_res4: [2, 4, 120, 160] -> out_res4_res2: [2, 4, 240, 320]\n",
    "            out_res4_res2 = F.interpolate(out_res4, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            B, _, H, W = out_res4_res2.shape\n",
    "\n",
    "            # samples: [B, 1, N, 2]\n",
    "            point_coords_res2, rows_int, cols_int = sample_points(out_res4_res2.detach(), gt_norm_mask,\n",
    "                                                                  sampling_ratio=self.sampling_ratio,\n",
    "                                                                  beta=self.importance_ratio)\n",
    "\n",
    "            # output (needed for evaluation / visualization)\n",
    "            out_res2 = out_res4_res2\n",
    "\n",
    "            # grid_sample feature-map\n",
    "            feat_res2 = F.grid_sample(x_d3, point_coords_res2, mode='bilinear', align_corners=True)  # (B, 256, 1, N)\n",
    "            init_pred = F.grid_sample(out_res4, point_coords_res2, mode='bilinear', align_corners=True)  # (B, 4, 1, N)\n",
    "            feat_res2 = torch.cat([feat_res2, init_pred], dim=1)  # (B, 256+4, 1, N)\n",
    "\n",
    "            # prediction (needed to compute loss)\n",
    "            samples_pred_res2 = self.out_conv_res2(feat_res2[:, :, 0, :])  # (B, 4, N)\n",
    "            samples_pred_res2 = norm_normalize(samples_pred_res2)  # (B, 4, N) - normalized\n",
    "\n",
    "            for i in range(B):\n",
    "                out_res2[i, :, rows_int[i, :], cols_int[i, :]] = samples_pred_res2[i, :, :]\n",
    "\n",
    "        else:\n",
    "            # grid_sample feature-map\n",
    "            feat_map = F.interpolate(x_d3, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            init_pred = F.interpolate(out_res4, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "            feat_map = torch.cat([feat_map, init_pred], dim=1)  # (B, 512+4, H, W)\n",
    "            B, _, H, W = feat_map.shape\n",
    "\n",
    "            out_res2 = self.out_conv_res2(feat_map.view(B, 256 + 4, -1))  # (B, 4, N)\n",
    "            out_res2 = norm_normalize(out_res2)  # (B, 4, N) - normalized\n",
    "            out_res2 = out_res2.view(B, 4, H, W)\n",
    "            samples_pred_res2 = point_coords_res2 = None\n",
    "\n",
    "        ################################################################################################################\n",
    "        # out_res1\n",
    "        ################################################################################################################\n",
    "\n",
    "        if mode == 'train':\n",
    "            # upsampling ... out_res4: [2, 4, 120, 160] -> out_res4_res2: [2, 4, 240, 320]\n",
    "            out_res2_res1 = F.interpolate(out_res2, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            B, _, H, W = out_res2_res1.shape\n",
    "\n",
    "            # samples: [B, 1, N, 2]\n",
    "            point_coords_res1, rows_int, cols_int = sample_points(out_res2_res1.detach(), gt_norm_mask,\n",
    "                                                                  sampling_ratio=self.sampling_ratio,\n",
    "                                                                  beta=self.importance_ratio)\n",
    "\n",
    "            # output (needed for evaluation / visualization)\n",
    "            out_res1 = out_res2_res1\n",
    "\n",
    "            # grid_sample feature-map\n",
    "            feat_res1 = F.grid_sample(x_d4, point_coords_res1, mode='bilinear', align_corners=True)  # (B, 128, 1, N)\n",
    "            init_pred = F.grid_sample(out_res2, point_coords_res1, mode='bilinear', align_corners=True)  # (B, 4, 1, N)\n",
    "            feat_res1 = torch.cat([feat_res1, init_pred], dim=1)  # (B, 128+4, 1, N)\n",
    "\n",
    "            # prediction (needed to compute loss)\n",
    "            samples_pred_res1 = self.out_conv_res1(feat_res1[:, :, 0, :])  # (B, 4, N)\n",
    "            samples_pred_res1 = norm_normalize(samples_pred_res1)  # (B, 4, N) - normalized\n",
    "\n",
    "            for i in range(B):\n",
    "                out_res1[i, :, rows_int[i, :], cols_int[i, :]] = samples_pred_res1[i, :, :]\n",
    "\n",
    "        else:\n",
    "            # grid_sample feature-map\n",
    "            feat_map = F.interpolate(x_d4, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            init_pred = F.interpolate(out_res2, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            feat_map = torch.cat([feat_map, init_pred], dim=1)  # (B, 512+4, H, W)\n",
    "            B, _, H, W = feat_map.shape\n",
    "\n",
    "            out_res1 = self.out_conv_res1(feat_map.view(B, 128 + 4, -1))  # (B, 4, N)\n",
    "            out_res1 = norm_normalize(out_res1)  # (B, 4, N) - normalized\n",
    "            out_res1 = out_res1.view(B, 4, H, W)\n",
    "            samples_pred_res1 = point_coords_res1 = None\n",
    "\n",
    "        return [out_res8, out_res4, out_res2, out_res1], \\\n",
    "               [out_res8, samples_pred_res4, samples_pred_res2, samples_pred_res1], \\\n",
    "               [None, point_coords_res4, point_coords_res2, point_coords_res1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2aa2fe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNET(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNET, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def get_1x_lr_params(self):  # lr/10 learning rate\n",
    "        return self.encoder.parameters()\n",
    "\n",
    "    def get_10x_lr_params(self):  # lr learning rate\n",
    "        return self.decoder.parameters()\n",
    "\n",
    "    def forward(self, img, **kwargs):\n",
    "        return self.decoder(self.encoder(img), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a6672257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "def load_checkpoint(fpath, model):\n",
    "    ckpt = torch.load(fpath, map_location='cpu')['model']\n",
    "\n",
    "    load_dict = {}\n",
    "    for k, v in ckpt.items():\n",
    "        if k.startswith('module.'):\n",
    "            k_ = k.replace('module.', '')\n",
    "            load_dict[k_] = v\n",
    "        else:\n",
    "            load_dict[k] = v\n",
    "\n",
    "    model.load_state_dict(load_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "43448e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "__imagenet_stats = {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}\n",
    "def unnormalize(img_in):\n",
    "    img_out = np.zeros(img_in.shape)\n",
    "    for ich in range(3):\n",
    "        img_out[:, :, ich] = img_in[:, :, ich] * __imagenet_stats['std'][ich]\n",
    "        img_out[:, :, ich] += __imagenet_stats['mean'][ich]\n",
    "    img_out = (img_out * 255).astype(np.uint8)\n",
    "    return img_out\n",
    "\n",
    "def kappa_to_alpha(pred_kappa):\n",
    "    alpha = ((2 * pred_kappa) / ((pred_kappa ** 2.0) + 1)) \\\n",
    "            + ((np.exp(- pred_kappa * np.pi) * np.pi) / (1 + np.exp(- pred_kappa * np.pi)))\n",
    "    alpha = np.degrees(alpha)\n",
    "    return alpha\n",
    "\n",
    "def concat_image(image_path_list, concat_image_path):\n",
    "    imgs = [Image.open(i).convert(\"RGB\").resize((640, 480), resample=Image.BILINEAR) for i in image_path_list]\n",
    "    imgs_list = []\n",
    "    for i in range(len(imgs)):\n",
    "        img = imgs[i]\n",
    "        imgs_list.append(np.asarray(img))\n",
    "\n",
    "        H, W, _ = np.asarray(img).shape\n",
    "        imgs_list.append(255 * np.ones((H, 20, 3)).astype('uint8'))\n",
    "\n",
    "    imgs_comb = np.hstack(imgs_list[:-1])\n",
    "    imgs_comb = Image.fromarray(imgs_comb)\n",
    "    imgs_comb.save(concat_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1e645156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device, results_dir):\n",
    "    alpha_max = 60\n",
    "    kappa_max = 30\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data_dict in test_loader:\n",
    "            print(\"Image Shape5: \", data_dict['img'].shape)\n",
    "            img = data_dict['img'].to(device)\n",
    "            print(\"Image Shape6: \", img.shape)\n",
    "            norm_out_list, _, _ = model(img)\n",
    "            norm_out = norm_out_list[-1]\n",
    "\n",
    "            pred_norm = norm_out[:, :3, :, :]\n",
    "            pred_kappa = norm_out[:, 3:, :, :]\n",
    "\n",
    "            # to numpy arrays\n",
    "            img = img.detach().cpu().permute(0, 2, 3, 1).numpy()                    # (B, H, W, 3)\n",
    "            pred_norm = pred_norm.detach().cpu().permute(0, 2, 3, 1).numpy()        # (B, H, W, 3)\n",
    "            pred_kappa = pred_kappa.cpu().permute(0, 2, 3, 1).numpy()\n",
    "\n",
    "            # save results\n",
    "            img_name = data_dict['img_name'][0].split(\"\\\\\")[1]\n",
    "\n",
    "            # 1. save input image\n",
    "            img = unnormalize(img[0, ...])\n",
    "\n",
    "            target_path = '%s/%s_img.png' % (results_dir, img_name)\n",
    "            plt.imsave(target_path, img)\n",
    "\n",
    "            # 2. predicted normal\n",
    "            pred_norm_rgb = ((pred_norm + 1) * 0.5) * 255\n",
    "            pred_norm_rgb = np.clip(pred_norm_rgb, a_min=0, a_max=255)\n",
    "            pred_norm_rgb = pred_norm_rgb.astype(np.uint8)                  # (B, H, W, 3)\n",
    "\n",
    "            target_path = '%s/%s_pred_norm.png' % (results_dir, img_name)\n",
    "            plt.imsave(target_path, pred_norm_rgb[0, :, :, :])\n",
    "\n",
    "            # 3. predicted kappa (concentration parameter)\n",
    "            target_path = '%s/%s_pred_kappa.png' % (results_dir, img_name)\n",
    "            plt.imsave(target_path, pred_kappa[0, :, :, 0], vmin=0.0, vmax=kappa_max, cmap='gray')\n",
    "\n",
    "            # 4. predicted uncertainty\n",
    "            pred_alpha = kappa_to_alpha(pred_kappa)\n",
    "            target_path = '%s/%s_pred_alpha.png' % (results_dir, img_name)\n",
    "            plt.imsave(target_path, pred_alpha[0, :, :, 0], vmin=0.0, vmax=alpha_max, cmap='jet')\n",
    "\n",
    "            # 5. concatenated results\n",
    "            image_path_list = ['img', 'pred_norm', 'pred_alpha']\n",
    "            image_path_list = ['%s/%s_%s.png' % (results_dir, img_name, i) for i in image_path_list]\n",
    "            target_path = '%s/%s_concat.png' % (results_dir, img_name)\n",
    "            concat_image(image_path_list, target_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1ea2bde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading checkpoint... datasets/scannet.pt\n",
      "Loading base model ()..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\phste/.cache\\torch\\hub\\rwightman_gen-efficientnet-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Removing last two layers (global_pool & classifier).\n",
      "loading checkpoint... / done\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "\n",
    "# load checkpoint\n",
    "checkpoint = 'datasets/scannet.pt'\n",
    "print('loading checkpoint... {}'.format(checkpoint))\n",
    "model = NNET().to(device)\n",
    "model = load_checkpoint(checkpoint, model)\n",
    "model.eval()\n",
    "print('loading checkpoint... / done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e70df5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomLoadPreprocess Ctor: \n",
      "CustomLoader Ctor: \n",
      "Shape: (480, 640, 3)\n",
      "Shape3: torch.Size([3, 480, 640])\n",
      "Shape4: torch.Size([3, 480, 640])\n",
      "Image Shape5:  torch.Size([1, 3, 480, 640])\n",
      "Image Shape6:  torch.Size([1, 3, 480, 640])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programme\\Anaconda\\envs\\python3.7\\lib\\site-packages\\ipykernel_launcher.py:27: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "d:\\Programme\\Anaconda\\envs\\python3.7\\lib\\site-packages\\ipykernel_launcher.py:17: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (480, 640, 3)\n",
      "Shape3: torch.Size([3, 480, 640])\n",
      "Shape4: torch.Size([3, 480, 640])\n",
      "Image Shape5:  torch.Size([1, 3, 480, 640])\n",
      "Image Shape6:  torch.Size([1, 3, 480, 640])\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "results_dir = './test-images' + '/results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "test_loader = CustomLoader('./test-images').data\n",
    "test(model, test_loader, device, results_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "d09c2806de043dea3ea18316e36902f2e7b94fe6e4b5908ef671554d3960daf7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
